{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import praw\n",
    "from prawcore import Forbidden\n",
    "from praw.exceptions import ClientException\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from pymongo import MongoClient, errors\n",
    "\n",
    "from bson.json_util import loads, dumps\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import boto3\n",
    "\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "import string\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = {}\n",
    "with open ('/opt/cap1/.cap1', 'r') as fp:\n",
    "    for line in fp:\n",
    "        k, v = line.replace('\\n','').split('\\t')\n",
    "        creds[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=creds['REDDIT_ID'], \n",
    "    client_secret=creds['REDDIT_SECRET'],\n",
    "    password=creds['REDDIT_PASSWORD'], \n",
    "    username=creds['REDDIT_USERNAME'],\n",
    "    user_agent='accessAPI:v0.0.1 (by /u/{})'.format(creds['REDDIT_USERNAME']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client['cap2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all comments into a list called 'docs', then make dataframe\n",
    "query = {}\n",
    "fields = { '_id': 0 }\n",
    "# docs is about 1.3 GB in RAM\n",
    "docs = list(db['comment'].find( query, fields ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = pd.DataFrame(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add submission ids to the set of ids from trolls, probably append 't1_' to comments\n",
    "# for ease of use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign labels, 'troll?', and 'child_of_troll?', and 'parent_of_troll?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of classes for target\n",
    "classes = ['other', 'parent', 'child', 'troll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6704"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign the label, 'troll?'' to each comment\n",
    "troll_comment_ids_set = set(comms[~comms['author'].isna()]['id'])\n",
    "comms['troll?'] = [int(mybool) for mybool in [\n",
    "                    commid in troll_comment_ids_set for commid in comms['id']\n",
    "                    ]]\n",
    "np.sum(comms['troll?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2985"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is this comment in reply to a troll?\n",
    "comms['child_of_troll?'] = [int(mybool) for mybool in [\n",
    "                    pid.split('_')[1] in troll_comment_ids_set for pid in comms['parent_id']\n",
    "                    ]]\n",
    "np.sum(comms['child_of_troll?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### only 57, praw never gave us children of troll comments...the only ones we have are from trolls replying to trolls\n",
    "\n",
    "#### FIXED, used praw to get the troll comments, then used .refresh() to load the replies. worked ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1824"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# did a troll reply to this?\n",
    "troll_parent_ids_set = set(\n",
    "                            [p.split('_')[1] for p in comms[~comms['author'].isna()]['parent_id']]\n",
    "                        )\n",
    "comms['parent_of_troll?'] = [int(myid in troll_parent_ids_set) for myid in comms['id']]\n",
    "\n",
    "np.sum(comms['parent_of_troll?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['t1', 't3'], dtype='<U2'), array([1839, 4348]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([p.split('_')[0] for p in np.unique(comms[~comms['author'].isna()]['parent_id'])], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hm, 1144 comments are parents of trolls, but there are 1839 't1'-style parent_ids among troll comments. Which of the unique parent ids aren't getting labeled as parent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6187"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_ids = np.unique(comms[~comms['author'].isna()]['parent_id'])\n",
    "len(parent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1839, 1824)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_parent_ids=[]\n",
    "for p in parent_ids:\n",
    "    if p[:2]=='t1':\n",
    "        t1_parent_ids.append(p.split('_')[1])\n",
    "set_parent_ids = set(t1_parent_ids)\n",
    "set_parent_of_troll = set(comms[comms['parent_of_troll?']==1]['id'])\n",
    "# how does a differ from \n",
    "len(set_parent_ids), len(set_parent_of_troll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the 695 comment ids that probably need to be set as parent_of_troll\n",
    "missing_commentids = np.array(t1_parent_ids)[[myid not in set_parent_of_troll for myid in t1_parent_ids]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([15]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are these comments simply missing from the data? YES\n",
    "# FIXED, only 15 remain after starting with 1824 ish\n",
    "num_hits = []\n",
    "for cid in missing_commentids:\n",
    "    num_hits.append(len(comms[comms['id']==cid]))\n",
    "np.unique(num_hits, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_commentids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printall(pd_obj):\n",
    "    '''\n",
    "    print every row and column in a pandas object\n",
    "    '''\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(pd_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author                                                                           NaN\n",
      "author_created_utc                                                               NaN\n",
      "author_flair_css_class                                                    mod-rikker\n",
      "author_flair_text                  http://steamcommunity.com/profiles/76561198054...\n",
      "author_fullname                                                             t2_6ey3m\n",
      "body                               Prepare yourself for several \"FTL for Borderla...\n",
      "controversiality                                                                   0\n",
      "created_utc                                                              1.34787e+09\n",
      "distinguished                                                                   None\n",
      "gilded                                                                             0\n",
      "id                                                                           c69bqat\n",
      "link_id                                                                    t3_100ikq\n",
      "nest_level                                                                       NaN\n",
      "parent_id                                                                  t3_100ikq\n",
      "reply_delay                                                                      NaN\n",
      "retrieved_on                                                                     NaN\n",
      "score                                                                              5\n",
      "stickied                                                                       False\n",
      "subreddit                                                                        NaN\n",
      "subreddit_id                                                                t5_2skv6\n",
      "created                                                                   1.3479e+09\n",
      "edited                                                                         False\n",
      "score_hidden                                                                   False\n",
      "can_gild                                                                        True\n",
      "is_submitter                                                                   False\n",
      "no_follow                                                                      False\n",
      "permalink                          /r/SteamGameSwap/comments/100ikq/h_borderlands...\n",
      "send_replies                                                                    True\n",
      "subreddit_type                                                            restricted\n",
      "gildings                                                                          {}\n",
      "updated_utc                                                                      NaN\n",
      "author_flair_template_id                                                        None\n",
      "collapsed                                                                      False\n",
      "collapsed_reason                                                                None\n",
      "total_awards_received                                                              0\n",
      "approved_at_utc                                                                  NaN\n",
      "ups                                                                                5\n",
      "awarders                                                                          []\n",
      "mod_reason_by                                                                    NaN\n",
      "banned_by                                                                        NaN\n",
      "author_flair_type                                                               text\n",
      "removal_reason                                                                   NaN\n",
      "likes                                                                            NaN\n",
      "user_reports                                                                      []\n",
      "saved                                                                          False\n",
      "banned_at_utc                                                                    NaN\n",
      "mod_reason_title                                                                 NaN\n",
      "archived                                                                        True\n",
      "can_mod_post                                                                   False\n",
      "report_reasons                                                                   NaN\n",
      "approved_by                                                                      NaN\n",
      "all_awardings                                                                     []\n",
      "downs                                                                              0\n",
      "author_flair_richtext                                                             []\n",
      "author_patreon_flair                                                           False\n",
      "body_html                          <div class=\"md\"><p>Prepare yourself for severa...\n",
      "associated_award                                                                 NaN\n",
      "author_premium                                                                 False\n",
      "author_flair_text_color                                                         dark\n",
      "num_reports                                                                      NaN\n",
      "locked                                                                         False\n",
      "name                                                                      t1_c69bqat\n",
      "treatment_tags                                                                    []\n",
      "subreddit_name_prefixed                                              r/SteamGameSwap\n",
      "depth                                                                              0\n",
      "author_flair_background_color                                                       \n",
      "collapsed_because_crowd_control                                                  NaN\n",
      "mod_reports                                                                       []\n",
      "mod_note                                                                         NaN\n",
      "_fetched                                                                        True\n",
      "author_cakeday                                                                   NaN\n",
      "top_awarded_type                                                                 NaN\n",
      "troll?                                                                             0\n",
      "child_of_troll?                                                                    0\n",
      "parent_of_troll?                                                                   0\n",
      "Name: 6704, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print the first non-troll comment, making sure to \n",
    "r = np.where(comms['author'].isna())\n",
    "printall(comms.iloc[r[0], :].iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission    :  14523\n",
      "first_and_last:    964\n",
      "comment       : 230853\n"
     ]
    }
   ],
   "source": [
    "for coll in db.list_collection_names():\n",
    "    print(f'{coll:14}: {db[coll].count_documents({}):6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c55cs4v,c5dwrdv,c69cm8a,c6eunvf,c78b3gw,c7b8vqn,c7bbduz,c7besrl,c7bguhc,c7c9krf,c7efzzm,c7his1t,c7hxdsi,c7i284w,c7if5z9,c7ikkce,c7iocqi,c7ipk1q,c7rzgab,c7wmj1b,c7wmk1b,c7wml2a,c8nt11a,c8ntmh0,c8o7nwz,cd714a1,cdw1dqk,ce1cp59,cestd6k,cfa9d26,cfaa1u5,cfffzjj,cffug3b,cfrt5a9,cfte0f6,cfthw86,cftmb63,cftn2sf,cftnpi7,cfto0bq,cftol0k,cftpl2v,cftptm9,cftzoeo,cg0z82u,cg1momq,cg5p2ru,cgfwynp,cgfxmpj,ch7qkl4,chx1tqc,cit9kt1,cla7hsy,cms6p22,cmx9dim,copkoou,cpfnvgt,cq9nusb,cqe9a6b,cqhtj4q,cqn2sk6,cqpo84t,crud3qp,crv6vb4,crv8y56,crv9qf9,crva5iw,crxk5ok,crxk9wh,crxmakj,cry9dbi,cry9lfz,cry9saj,cryaar6,crybk2d,cryc2jn,crycsmf,crye6rs,cryenpi,cryf4xc,cryfckw,cryjjtr,cs0t3uw,cs39w0u,cs43hro,cs622nd,csbt0mq,csh8dik,cshboie,csoor1q,cszkuf2,cszl0hb,cszl5re,ctd7bv4,cudpmov,cuec9uh,cuewltu,cufl8js,cujkrrl,cujwdox,cuui4li,cv9xyrn,cvfzk0q,cvh4ckj,cvh58gr,cvy704o,cw5aipy,cwas0ak,cwb1g5f,cwzywoo,cx0gufr,cx4g56i,cx7u0p5,cx7uqso,cx7vugn,cx8ombk,cx90vp5,cxceyio,cxcf6ts,cxcflk3,cxcfxef,cxcgeqx,cxcglff,cxcgxu9,cxcj30j,cxckc8r,cxdmsbp,cxdolz3,cxdomw5,cxdpjbn,cxdpsu9,cxdpwhv,cxe0dsg,cxe3c8l,cxeopar,cxeozeb,cxeqtx0,cxezer1,cxfmpf0,cxfn1ez,cxfnjxu,cxfnl0e,cxfnmyr,cxfo84h,cxfor4b,cxforu0,cxfovrw,cxfow11,cxfp380,cxhbj35,cxhrw7u,cxhsuha,cxhszne,cxhtluk,cxhtmdf,cxhtq9r,cxhtwsu,cxhu1i9,cxhuf1h,cxhuok6,cxhv0uk,cxhv2vc,cxhv6nv,cxhvzms,cxhxdcj,cxhxkdx,cxhxqy2,cxhxx65,cxk9v9j,cxk9za5,cxkai82,cxkaoa2,cxkc4u5,cxkc765,cxkgsi1,cxl06g5,cxlecsp,cxlffw9,cxlgg8p,cxlj3ic,cxlj5ab,cxljevf,cxljh5x,cxlmech,cxlob7i,cxvaagv,cz083mi,cz0c7qt,cz2bxw8,cz2en7e,cz2femm,cz2fyp2,cz2gmpz,cz5z8hw,cz5ziuc,cz60gm8,cz692ow,cz69bzj,cz69pu0,cz6a2wt,cz6a6hj,cz6abjb,cz6akeo,cz6bacz,cz6bflo,cz6buu4,cz6wcx5,cz7lwer,cz8997n,cz8anof,cz8duyo,cz8mx6g,cz8ogao,cz8opm7,cz8vlfk,cz974zo,cz9r22e,cz9szyu,czan9b7,czbqmmo,czbtl34,czbz9s2,czbzskb,czc0225,czc0m7l,d0pwchu,d14gnh5,d14isxw,d14la0s,d151gi4,d17gzg5,d19bdrf,d19dybw,d19fbla,d1alskg,d1b8x0k,d1c04so,d1cxwkd,d1cztvy,d1d3nxd,d1e2i1r,d1e3a1q,d1e3e2l,d1e4qke,d1e6azh,d1hrc5v,d1j4rae,d1j9t1y,d1jfawb,d1jzm65,d1krmzt,d1ll4ia,d1ll67u,d1lm0t0,d1ltmi4,d1luxnk,d1lv6n9,d1lw1nw,d1madm8,d1mw3uj,d1mwne0,d1n21ki,d1p9zsq,d1pbgju,d1pc1sy,d1pc5iv,d1pctfs,d1pnovt,d1pxe0w,d1qend9,d1qj6vw,d1qm5jk,d1qmynf,d1qo5oa,d1qosqj,d1qoutf,d1qp6ro,d1qpc41,d1qpsr4,d1qquh4,d1qykna,d1s2vju,d1s3t88,d1sqoqw,d1ssmus,d1ssnvn,d1suu5e,d1svy2m,d1syxqc,d1szmy6,d1t64p2,d1t7hqx,d1tb3bh,d1tb3l3,d1tb7fv,d1tbcfy,d1tbh53,d1tbhea,d1tbi13,d1tbibu,d1tbj96,d1tbket,d1tblp8,d1tbnzi,d1tbog0,d1tbp59,d1tbpig,d1tbq1f,d1tbrj8,d1tbs02,d1tbu2b,d1tbuns,d1tc5h3,d1tcii4,d1tcxzx,d1tfjfk,d1tgk0n,d1thso3,d1ts3za,d1u569q,d1ut0lx,d1uvooa,d1uw4ts,d1uwpnw,d1uxnub,d1v6z41,d1vcll6,d1vgjno,d1vvaao,d1w11xq,d1w15dr,d1w1bkp,d1y4hhk,d1y9wrg,d1ycho5,d1yemp8,d2080rk,d20ckdv,d20fh3p,d217xia,d22ptpl,d22puur,d23zvbo,d2491rx,d252rqi,d252yzr,d253z4h,d254yzw,d261vye,d2641p2,d264oa4,d266hsh,d26oqer,d26rpag,d277jpu,d2791gg,d27brph,d27h0pw,d27lszf,d28v4nd,d2auuo9,d2auxwg,d2aycvt,d2d49le,d2dfyls,d2drr4z,d2dtune,d2dx02b,d2e5do8,d2e71d8,d2elerg,d2enczy,d2eonu7,d2exlqr,d2f5sr2,d2g5ncr,d2g9no4,d2ga823,d2gbwxx,d2gcdhc,d2h35ov,d2i138n,d2ic0xz,d2id4g6,d2it95t,d2j6e3y,d2j76l2,d2j7e7r,d2j8ekg,d2j9b2f,d2j9ix9,d2j9xh8,d2jbrgr,d2jda7o,d2jnt83,d2jtban,d2jvmpz,d2lc5v4,d2ldsqe,d2lf8ni,d2lvdgt,d2lvmd2,d2lvmvi,d2lvvzh,d2lw5z0,d2lwbuk,d2mklde,d2mu0k6,d2mwww0,d2njoj2,d2o2g0q,d2otx1a,d2pzo5h,d2q41yv,d2sxssa,d2syjaf,d2t0w5r,d2t67o5,d2vn6i2,d2xqo5h,d2y02au,d31g2m5,d31gzr8,d32b1sf,d32bqm8,d32iexp,d32qsb6,d32rhcu,d3328wl,d33y8rr,d34as3o,d37nmjc,d37wn91,d39mb0i,d3cqodd,d3folo7,d3fuvll,d3g1x7j,d3g2bif,d3g2jw9,d3g2uci,d3g43kj,d3g4ofb,d3g4zn0,d3g5zm5,d3g6f4b,d3g6m0e,d3g766h,d3hq6dv,d3hrea2,d3iiv18,d3j23jv,d3j8few,d3mzkxa,d3okaww,d3oqa8p,d3otx6h,d3oxkqg,d3oxscu,d3oy4s0,d3p18pf,d3p19cq,d3p1bbj,d3plvjb,d3pm6vm,d3pmp21,d3pqyqx,d3pu06a,d3pum7a,d3pwtf5,d3pxzdm,d3py6mi,d3pzn2s,d3pzy6o,d3q1t5e,d3q29l1,d3q2krl,d3q342s,d3q5703,d3q83i4,d43fi9t,d45nk4k,d4a87es,d4bsxr2,d4by6g0,d4cb2v8,d4g0kpt,d4g4j3x,d4h1alo,d4mlx05,d4mqncj,d4n2c11,d4nbbch,d4nbf0l,d4p2ucs,d4qdch5,d4qt7ql,d4quadd,d4quv2y,d4rvfnj,d4ssri9,d4stfck,d4sutrk,d4yzv4c,d4z26z2,d508xpp,d586t07,d58ixge,d5a8ka0,d5a8l7b,d5arr2l,d5eapz0,d5egc61,d5jv3px,d5jv62p,d5krskk,d5lxmxl,d5lxsrn,d5mi32l,d5mofax,d5n1bbu,d5n1ho8,d5nhvc8,d5q7ccw,d5q7r91,d5q8dxn,d5q9mhf,d5qfts5,d5qjtw3,d5r1rzi,d5r8gcs,d5rdfmb,d5s18x6,d5uadra,d5uajeo,d5uaur0,d5uuq2f,d5uz1ed,d61lyfb,d63ds1w,d64kbwv,d64x6un,d66qvb3,d6954qv,d69ew7r,d69juof,d6arup5,d6axze8,d6bycyr,d6c8yqg,d6ckldh,d6ctlyc,d6eulyn,d6h6jt8,d6h6n5w,d6j4pwt,d6jks0l,d6jl7j3,d6k7yks,d6kj5v5,d6kvy45,d6l0guj,d6l0u8u,d6l165u,d6l1kt2,d6l1t7i,d6l27s8,d6l2bte,d6l6smb,d6l75eu,d6l7apo,d6l8ysl,d6mracy,d6mtjvs,d6mu2kd,d6mwfkb,d6o1y4x,d6otyug,d6pjqev,d6qka6z,d6rt3gs,d6rv4lm,d6rwb6k,d6rz5cl,d6tudse,d6usax0,d6w1ja2,d6wlyr5,d6x1okd,d6ynfzl,d73rcxe,d7aorye,d7bo811,d7bu3og,d7bv0tb,d7c2sso,d7c57jl,d7c82wu,d7cbptr,d7cd7h1,d7coseh,d7cyy5v,d7d24oi,d7dh122,d7dk7hh,d7dph46,d7e7e81,d7e8271,d7ehh33,d7gla4j,d7vsvcf,d82thc3,d82v1yp,d82y9vq,d831vmh,d859lds,d86opv2,d89ksxj,d89q3g9,d8apklu,d8eo5ua,d8g8zqr,d8gwsuz,d8gx6oz,d8gxkd6,d8qegac,d8rjwkq,d8ufgac,d8ufyxm,d8uhl3l,dac8p23,dc4c6pn,dd5b62b,ddep5tv,ddit6nd,ddj1h1g,deshi9g,dgji9p2,dgjlux5,dgs7oeb,dgxohi0,dh1x3u6,dh5dw18,dh5g7dw,dh9otwm,dh9pcgz,dh9pgdz,dh9pxym,dh9qa3f,dh9qnbe,dheir8r,dhfxr33,dhfxxy7,dhnt9w8,dhnub5d,dhnuq90,dhnv8wt,dhnvb13,dhovdw5,dhovv2n,dhowrg7,dhozjjh,dhpxevw,dhr1bdt,dhr399k,dhzjpxa,diq0sb8,dk5x3ny,dk8qhos,dmatz6g,dmk6hhx,dmk79uq,dmk7rxn,dmlpzjt,dmr41zp,dndeu8p,dndiu8i,doym0y6,dp9fs27,dpu7uy3,dpuc6ez,drlesmy,drlfrf3,drmdpxc,drqqc9v,drqqde3,drs6lj0,drwjoag,ds8tvfr,dsc2qbw,dsdhf3h,dsirfdb,dtijtnq,dtikbzb,dtminat'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(missing_commentids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_fetched': False,\n",
      " '_reddit': <praw.reddit.Reddit object at 0x7f05277a2b50>,\n",
      " '_replies': [],\n",
      " '_submission': None,\n",
      " 'id': 'cqe9a6b'}\n"
     ]
    }
   ],
   "source": [
    "# Reddit's api changed, dict(vars(comment)) only worked after running a ._fetch()\n",
    "# get missing comments from pushshift\n",
    "# gen = api.search_comments(id=','.join(missing_commentids))\n",
    "# gen = api.search_comments(id='c55cs4v')\n",
    "ids =','.join(missing_commentids[:2])\n",
    "#print(ids)\n",
    "#gen = api.search_comments(id=ids)\n",
    "comment = reddit.comment(id=\"cqe9a6b\")\n",
    "pprint(vars(comment))\n",
    "#for c in gen:\n",
    "#    print(c.d_['body'])\n",
    "    #db['comment'].insert_one(c.d_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_replies': [],\n",
       " '_submission': None,\n",
       " '_reddit': <praw.reddit.Reddit at 0x7fdd967f7970>,\n",
       " '_fetched': False,\n",
       " 'id': 'c551ecd'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://snew.notabug.io/r/unixporn/comments/vjbgg/archlinux_dwm/c551ecd/\n",
    "# comment = reddit.comment(id=\"c55cs4v\")\n",
    "comment = reddit.comment(id=\"c551ecd\")\n",
    "# comment = reddit.comment(id=\"cqe9a6b\")\n",
    "dict(vars(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, so c55cs4v is here -- \n",
    "# https://snew.notabug.io/r/unixporn/comments/vjbgg/archlinux_dwm/c55cs4v/\n",
    "# and you can see shomyo's reply with permalink -- \n",
    "# https://snew.notabug.io/r/unixporn/comments/vjbgg/archlinux_dwm/c55hjc4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_replies': [],\n",
       " '_submission': None,\n",
       " '_reddit': <praw.reddit.Reddit at 0x7fdd967f7970>,\n",
       " '_fetched': True,\n",
       " 'id': 'c551ecd',\n",
       " 'total_awards_received': 0,\n",
       " 'approved_at_utc': None,\n",
       " 'edited': False,\n",
       " 'mod_reason_by': None,\n",
       " 'banned_by': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'removal_reason': None,\n",
       " 'link_id': 't3_vjbgg',\n",
       " 'author_flair_template_id': None,\n",
       " 'likes': None,\n",
       " 'user_reports': [],\n",
       " 'saved': False,\n",
       " 'banned_at_utc': None,\n",
       " 'mod_reason_title': None,\n",
       " 'gilded': 0,\n",
       " 'archived': True,\n",
       " 'no_follow': True,\n",
       " 'author': Redditor(name='ProbableRepost'),\n",
       " 'can_mod_post': False,\n",
       " 'created_utc': 1340577203.0,\n",
       " 'send_replies': True,\n",
       " 'parent_id': 't3_vjbgg',\n",
       " 'score': 7,\n",
       " 'author_fullname': 't2_5w9yd',\n",
       " 'approved_by': None,\n",
       " 'mod_note': None,\n",
       " 'all_awardings': [],\n",
       " 'subreddit_id': 't5_2sx2i',\n",
       " 'body': \"Finally some decent font rendering on a non-Ubuntu setup. Care to share?\\n\\nHowever, maybe it's the resolution, but text seems all-round too big for my taste. Also, I'm not a huge fan of the textured wallpaper.\\n\\nThe colours are consistent, however, and I like the cyan accents, making for generally likable desktop. Good job. 8/10\",\n",
       " 'awarders': [],\n",
       " 'author_flair_css_class': None,\n",
       " 'name': 't1_c551ecd',\n",
       " 'author_patreon_flair': False,\n",
       " 'downs': 0,\n",
       " 'author_flair_richtext': [],\n",
       " 'is_submitter': False,\n",
       " 'body_html': '<div class=\"md\"><p>Finally some decent font rendering on a non-Ubuntu setup. Care to share?</p>\\n\\n<p>However, maybe it&#39;s the resolution, but text seems all-round too big for my taste. Also, I&#39;m not a huge fan of the textured wallpaper.</p>\\n\\n<p>The colours are consistent, however, and I like the cyan accents, making for generally likable desktop. Good job. 8/10</p>\\n</div>',\n",
       " 'gildings': {},\n",
       " 'collapsed_reason': None,\n",
       " 'distinguished': None,\n",
       " 'associated_award': None,\n",
       " 'stickied': False,\n",
       " 'author_premium': False,\n",
       " 'can_gild': True,\n",
       " 'top_awarded_type': None,\n",
       " 'author_flair_text_color': None,\n",
       " 'score_hidden': False,\n",
       " 'permalink': '/r/unixporn/comments/vjbgg/archlinux_dwm/c551ecd/',\n",
       " 'num_reports': None,\n",
       " 'locked': False,\n",
       " 'report_reasons': None,\n",
       " 'created': 1340606003.0,\n",
       " 'subreddit': Subreddit(display_name='unixporn'),\n",
       " 'author_flair_text': None,\n",
       " 'treatment_tags': [],\n",
       " 'collapsed': False,\n",
       " 'subreddit_name_prefixed': 'r/unixporn',\n",
       " 'controversiality': 0,\n",
       " 'author_flair_background_color': None,\n",
       " 'collapsed_because_crowd_control': None,\n",
       " 'mod_reports': [],\n",
       " 'subreddit_type': 'public',\n",
       " 'ups': 7}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment._fetch()\n",
    "dict(vars(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top-up the parent comments, worked great.\n",
    "log = [] \n",
    "i = 0 # had clientexception at 59, id='cqe9a6b'\n",
    "for comment_id in missing_commentids[i:]: \n",
    "    i += 1 \n",
    "    print(f'searching for comment id: {comment_id}') \n",
    "    comment = reddit.comment(id=comment_id) \n",
    "    try: \n",
    "        comment._fetch() \n",
    "    except (Forbidden, ClientException): \n",
    "        log.append(i) \n",
    "        continue \n",
    "    d = dict(vars(comment)) \n",
    "    for key in ['_replies', '_submission', \n",
    "            '_reddit', 'mod', 'author', \n",
    "            'subreddit']: \n",
    "        _ = d.pop(key, None) \n",
    "    try: \n",
    "        db['comment'].insert_one(d) \n",
    "    except (errors.DuplicateKeyError, errors.InvalidDocument): \n",
    "        log.append[i] \n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission    :  14523\n",
      "first_and_last:    964\n",
      "comment       : 231533\n"
     ]
    }
   ],
   "source": [
    "for coll in db.list_collection_names():\n",
    "    print(f'{coll:14}: {db[coll].count_documents({}):6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the before and after numbers of records in mongo\n",
    "# 15 records failed\n",
    "num_records_added = 231533 - 230853\n",
    "num_records_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cqhtj4q', 'crv6vb4', 'cs43hro', 'cujkrrl', 'cwzywoo', 'cz0c7qt',\n",
       "       'd4ssri9', 'd6rt3gs', 'dd5b62b', 'dk5x3ny', 'dmk6hhx', 'dndiu8i',\n",
       "       'dp9fs27', 'drqqc9v', 'dsc2qbw'], dtype='<U7')"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from the ipython session in which I ran the scrape\n",
    "log = [59, 63, 84, 98, 109, 187, 500, 580, 632, 667, 670, 676, 678, 684, 689]\n",
    "missing_commentids[log]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cqhtj4q\n",
    "```\n",
    "[–]zenzog3 points 5 years ago \n",
    "https://mobile.twitter.com/shoxCSGO/status/588523685419601922\n",
    "permalink save report give gold reply\n",
    "```\n",
    "https://snew.notabug.io/r/csgobetting/comments/3324gu/envyus_vs_dignitas_bo3_190415_2100_cest/cqhtj4q/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d4ssri9\n",
    "```\n",
    "[–][censored]3 points 4 years ago \n",
    "[censored]\n",
    "permalink save report give gold reply[removed by moderators]\n",
    "```\n",
    "https://snew.notabug.io/r/rage/comments/4qgm68/woman_26_wielded_hatchet_after_her_demands_for/d4ssri9/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dp9fs27\n",
    "```\n",
    "–][deleted]2 points 3 years ago \n",
    "[deleted]\n",
    "permalink save report give gold reply\n",
    "```\n",
    "https://snew.notabug.io/r/Sissies/comments/7ad3vx/any_requests/dp9fs27/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d2j7hcz'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the replies to the trolls\n",
    "# get missing comments from pushshift\n",
    "troll_commentids = comms[~comms['author'].isna()]['id']\n",
    "np.random.choice(troll_commentids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comment(id='d659sbh')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment = reddit.comment(np.random.choice(troll_commentids))\n",
    "comment.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_replies': <praw.models.comment_forest.CommentForest object at 0x7fdd76ff6940>, '_submission': Submission(id='4wa6mn'), '_reddit': <praw.reddit.Reddit object at 0x7fdd967f7970>, 'total_awards_received': 0, 'approved_at_utc': None, 'ups': 1, 'awarders': [], 'mod_reason_by': None, 'banned_by': None, 'author_flair_type': 'text', 'removal_reason': None, 'link_id': 't3_4wa6mn', 'author_flair_template_id': None, 'likes': None, 'user_reports': [], 'saved': False, 'id': 'd65ber0', 'banned_at_utc': None, 'mod_reason_title': None, 'gilded': 0, 'archived': True, 'no_follow': True, 'author': Redditor(name='kempff'), 'can_mod_post': False, 'send_replies': True, 'parent_id': 't1_d659sbh', 'score': 1, 'author_fullname': 't2_6ayqq', 'report_reasons': None, 'approved_by': None, 'all_awardings': [], 'subreddit_id': 't5_2qil2', 'collapsed': True, 'body': \"There is a possibility it just did. I noticed the wife was not interviewed. I'm keeping an eye on this story to see if it turns out to be a hoax.\", 'edited': False, 'author_flair_css_class': None, 'is_submitter': False, 'downs': 0, 'author_flair_richtext': [], 'author_patreon_flair': False, 'body_html': '<div class=\"md\"><p>There is a possibility it just did. I noticed the wife was not interviewed. I&#39;m keeping an eye on this story to see if it turns out to be a hoax.</p>\\n</div>', 'gildings': {}, 'collapsed_reason': None, 'associated_award': None, 'stickied': False, 'author_premium': False, 'subreddit_type': 'public', 'can_gild': True, 'top_awarded_type': None, 'author_flair_text_color': None, 'score_hidden': False, 'permalink': '/r/rage/comments/4wa6mn/black_firefighters_home_burnsup_after_receiving/d65ber0/', 'num_reports': None, 'locked': False, 'name': 't1_d65ber0', 'created': 1470432685.0, 'subreddit': Subreddit(display_name='rage'), 'author_flair_text': None, 'treatment_tags': [], 'created_utc': 1470403885.0, 'subreddit_name_prefixed': 'r/rage', 'controversiality': 0, 'depth': 1, 'author_flair_background_color': None, 'collapsed_because_crowd_control': None, 'mod_reports': [], 'mod_note': None, 'distinguished': None, '_fetched': True}\n"
     ]
    }
   ],
   "source": [
    "for reply in comment._replies:\n",
    "    print(dict(vars(reply)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission    :  14523\n",
      "first_and_last:    964\n",
      "comment       : 231533\n"
     ]
    }
   ],
   "source": [
    "for coll in db.list_collection_names():\n",
    "    print(f'{coll:14}: {db[coll].count_documents({}):6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-up the child comments\n",
    "log = [] \n",
    "i = 0 \n",
    "for comment_id in troll_commentids:  \n",
    "    i += 1  \n",
    "    print(f'searching for comment id: {comment_id}')  \n",
    "    comment = reddit.comment(id=comment_id)  \n",
    "    try:  \n",
    "        comment.refresh() \n",
    "    except (Forbidden, ClientException):  \n",
    "        log.append(i)  \n",
    "        continue \n",
    "    for reply in comment._replies: \n",
    "        d = dict(vars(reply)) \n",
    "        did = d['id'] \n",
    "        print(f'found reply id: {did}')  \n",
    "        for key in ['_replies', '_submission',  \n",
    "                '_reddit', 'mod', 'author',  \n",
    "                'subreddit']:  \n",
    "            _ = d.pop(key, None)  \n",
    "        try:  \n",
    "            db['comment'].insert_one(d) \n",
    "        except (errors.DuplicateKeyError, errors.InvalidDocument): \n",
    "            log.append(i)  \n",
    "            continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission    :  14523\n",
      "first_and_last:    964\n",
      "comment       : 234008\n"
     ]
    }
   ],
   "source": [
    "for coll in db.list_collection_names():\n",
    "    print(f'{coll:14}: {db[coll].count_documents({}):6}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_stopwords_ = set(\"a,able,about,across,after,all,almost,also,am,among,an,and,any,\\\n",
    "are,as,at,be,because,been,but,by,can,could,dear,did,do,does,either,\\\n",
    "else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,\\\n",
    "how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,\\\n",
    "me,might,most,must,my,neither,no,of,off,often,on,only,or,other,our,\\\n",
    "own,rather,said,say,says,she,should,since,so,some,than,that,the,their,\\\n",
    "them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,\\\n",
    "what,when,where,which,while,who,whom,why,will,with,would,yet,you,your,u,s\".split(','))\n",
    "\n",
    "# new stopwords after first run through\n",
    "# sws_to_add_brf1 = 'thanks,op,tie,deleted,game,crypto,invest,\\\n",
    "# edit,platform,play,giveaway,ok,yeah,blockchain,enter,remove,\\\n",
    "# m,token,awesome,ethereum,exchange,steam,dude'.split(',')\n",
    "\n",
    "# stopwords_ = set(list(orig_stopwords_) + sws_to_add_brf1)\n",
    "stopwords_ = orig_stopwords_\n",
    "\n",
    "punctuation_ = set(string.punctuation + '’' + '“')\n",
    "\n",
    "def rm_punctuation(a_string):\n",
    "    table = str.maketrans('', '', ''.join(punctuation_))\n",
    "    return a_string.translate(table)\n",
    "        \n",
    "def tokens_lower(tokens):\n",
    "    return [word.lower() for word in tokens]\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "#     for w in sent: print(w)\n",
    "#     if w == \"’\": print(\"this is ': \" + w)\n",
    "    return [w for w in tokens if not w in stopwords_]\n",
    "\n",
    "def stem_tokens(ntlk_stem_obj, tokens):\n",
    "    # visualize what stemming and lemmitization does!\n",
    "    # str(porter.__class__) = \"<class 'nltk.stem.porter.PorterStemmer'>\"\n",
    "#     name = str(ntlk_stem_lemm_obj.__class__).split(\"'\")[1].split('.')[-1]\n",
    "#     count_ident = 0\n",
    "#     count_alter = 0\n",
    "    return [ntlk_stem_obj.stem(tok) for tok in tokens]\n",
    "#     elif ''\n",
    "# for tok in tokens:\n",
    "#         return ntlk_stem_lemm_obj.stem(tok)\n",
    "#         if tok == stem_lemm_f_l_tok:\n",
    "#             count_ident += 1\n",
    "#         else:\n",
    "#             print(tok, stem_lemm_f_l_tok)\n",
    "#             count_alter += 1\n",
    "#     print('{}:\\nNumber of unchanged words: {}\\nchanged words: {}\\n'.format(name, count_ident, count_alter))\n",
    "\n",
    "def lemm_tokens(ntlk_lemm_obj, tokens):\n",
    "    return [ntlk_lemm_obj.lemmatize(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_pre_proc_doc(input_string, stemming=False):\n",
    "    '''\n",
    "    given document:\n",
    "        tokenizes the document\n",
    "        sets tokens to lower case\n",
    "        filters punctuation and stop words from tokens\n",
    "        returns porter, snowball, and wordnet stem/lemm tokens\n",
    "    '''\n",
    "    no_punct_string = rm_punctuation(input_string)\n",
    "    tokens = word_tokenize(no_punct_string)\n",
    "    lo_tokens = tokens_lower(tokens)\n",
    "    f_lo_tokens = filter_tokens(lo_tokens)\n",
    "    if stemming:\n",
    "        s_f_lo_tokens = stem_tokens(porter, f_lo_tokens)\n",
    "        le_s_f_lo_tokens = lemm_tokens(wordnet, s_f_lo_tokens)\n",
    "        return ' '.join(le_s_f_lo_tokens)\n",
    "    else:\n",
    "        le_f_lo_tokens = lemm_tokens(wordnet, f_lo_tokens)\n",
    "        return ' '.join(le_f_lo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a few minutes, and uses about 100 MB of RAM\n",
    "corpus = [nlp_pre_proc_doc(d) for d in comms['body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a dictionary of trained classifiers for comparison\n",
    "clfs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_corp, X_test_corp, y_train, y_test = train_test_split(\n",
    "        corpus, y, test_size=0.2, random_state=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 5000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_corp)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_tfidf.toarray()\n",
    "X_test = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "# at first I did 100 estimators, but 100*25 is only 2500 \n",
    "#  whereas we have 5000 features in tfidf. Increase to 400\n",
    "model_param = {'n_estimators': 400,\n",
    "                   'max_depth': 5,\n",
    "                   'max_features': 25,\n",
    "                   'oob_score': True,\n",
    "                   'n_jobs': -1,\n",
    "                   'random_state': 30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brf = RandomForestClassifier(**model_param, class_weight='balanced_subsample')\n",
    "brf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbrf = BalancedRandomForestClassifier(**model_param, class_weight='balanced_subsample')\n",
    "imbrf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_confusion_matrix(y_true, y_predict):\n",
    "    \"\"\"\n",
    "    y_true = [1, 1, 1, 1, 1, 0, 0]\n",
    "\n",
    "    y_predict = [1, 1, 1, 1, 0, 0, 0]\n",
    "\n",
    "    In [1]: standard_confusion_matrix(y_true, y_predict)\n",
    "    >> array([[4., 1.],\n",
    "    >>       [0., 2.]])\n",
    "    \"\"\"\n",
    "    cm = np.zeros((2,2))\n",
    "    X = np.array([y_true, y_predict])\n",
    "    values, counts = np.unique(X, axis=1, return_counts=True)\n",
    "    for i, v in enumerate(values.T):\n",
    "        cm[tuple([1, 1] - v)] = counts[i]\n",
    "    return cm.T.astype(int)\n",
    "\n",
    "# from the lecture\n",
    "# Just handy function to make our confusion matrix pretty \n",
    "def plot_confusion_matrix(cm, # confusion matrix\n",
    "                          classes_x, # test to describe what the output of the classes may be (commonly 1 or 0)\n",
    "                          classes_y,\n",
    "                          normalize=False, \n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes_x))\n",
    "    plt.xticks(tick_marks, classes_x, rotation=45)\n",
    "    plt.yticks(tick_marks, classes_y)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i,  format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Predicted label')\n",
    "    plt.xlabel('True label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_nofit(ax, X_test, y_test, clf, clf_name, **kwargs):\n",
    "    y_prob = np.zeros((len(y_test),2))\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    # Predict probabilities, not classes\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob[:, 1])\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if len(ax.lines) == 0:\n",
    "        plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "    ax.plot(fpr, tpr, lw=1, label='%s (area = %0.2f)' % (clf_name, roc_auc))\n",
    "    mean_tpr /= 1\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "#     plt.plot(mean_fpr, mean_tpr, 'k--',label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the model for comparisons\n",
    "clfs['Balanced_RF'] = brf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(brf, X_test, y_test,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rnd_smp = np.random.random_sample(len(X)) < 0.\n",
    "#fig, ax = plt.subplots(1, figsize=(6, 5))\n",
    "#classifier_labels = ['Random_Forest (RF)', 'Balanced_RF']\n",
    "#classifiers = {'Random_Forest (RF)': rf, 'Balanced_RF': brf}\n",
    "#for label, clf in clfs.items():\n",
    "    #plot_roc_nofit(ax, X_test, y_test, clf, label)\n",
    "    #multi_class='ovo'\n",
    ">>> import numpy as np\n",
    ">>> from sklearn.metrics import roc_auc_score\n",
    ">>> y_true = np.array([0, 0, 1, 1])\n",
    ">>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    ">>> roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "troll_comments = comms[comms['troll?']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(troll_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the ids for all the troll_comments, then retrieve all the replies to it from Pushshift\n",
    "troll_comments['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = api.search_comments(parent_id='t1_dr3b6ce,t1_cpe9ci5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gen).d_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen = api.search_comments(id='cax4ng0,cax1t9x')\n",
    "next(gen).d_, next(gen).d_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so, let's recap the tomfoolery between praw and psaw\n",
    "0) psaw returns comments from trolls, praw does not ... true? FALSE, psaw doesn't give results when parent_id is from a troll\n",
    "1) psaw returns nothing when searching on threads with comments from trolls ... true? mostly. I believe it's not 100%, but e.g. https://api.pushshift.io/reddit/submission/comment_ids/4rdu5x returns []\n",
    "2) re: 1, reddit does, but does not include any responses to the trolls ... true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "* classify on pca / increase the number of maximum features per tree\n",
    "* data completeness\n",
    "  - get nest_level for everything\n",
    "  - do we have all comments multiple levels below troll comments?\n",
    "  - authors, subreddits, link- and comment-karma\n",
    "  - permalink\n",
    "* one-hot encode the subreddit to add to the tfidf\n",
    "* add user activity profile to the tfidf\n",
    "* start with id, (comment)\n",
    "  - is the comment trollish? proba > threshold\n",
    "  - if yes, then \n",
    "    - classify all other comments from the user (max=1000?)\n",
    "    - for each trollish comment, recurse on other trollish comments in its thread (max=1000?)\n",
    "  - given all the comments (including non-trollish from threads), view the author creation date and test for spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    troll_comments = comms[comms['troll?']==1]\n",
    "    print(troll_comments[troll_comments['subreddit']=='politics'])\n",
    "# comms.tail().iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    not_troll_comments = comms[comms['troll?']!=1]\n",
    "    print(not_troll_comments[not_troll_comments['subreddit_id']=='t5_2cneq'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = comms['class_label'].values\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(comms['subreddit_id'], comms['subreddit']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(comms['subreddit_id'] + '_' + comms['subreddit'], return_counts=True)\n",
    "comms['subreddit_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join the [71.6 million usernames](https://old.reddit.com/r/pushshift/search?q=karma&restrict_sr=on&sort=relevance&t=all) to the user activity dataframe so we can efficiently load the link and comment karma of all these users into mongo\n",
    "```bash\n",
    "comm <(\\\n",
    "    sed '1d' user_activity_dataframe.csv | cut -d, -f2 | sort -u\\\n",
    "    ) \\\n",
    "    <(\\\n",
    "    cut -d, -f1 user_activity_dataframe_reddit_accounts.csv | sort -u\\\n",
    "    ) | \n",
    "awk -F\"\\t\" '{print NF;}' | sort | uniq -c\n",
    "```\n",
    "``` \n",
    "    56 1\n",
    "   277 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the user link- and comment-karma from reddit praw\n",
    "for item in reddit.redditor(\"Kevin_Milner\").downvoted():\n",
    "    print(item.id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit.redditor(\"spez\")\n",
    "reddit.user.me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.redditor(\"spez\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(reddit.redditor(fullname=\"t2_z919g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.subreddit(display_name=\"t5_2r99w\").subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = reddit.subreddits.search(\"t5_2r99w,t5_2r99w\")\n",
    "\n",
    "# assume you have a Reddit instance bound to variable `reddit`\n",
    "# str(reddit.submission(id=\"39zje0,\").subreddit)\n",
    "# print(submission.title) # to make it non-lazy\n",
    "# pprint.pprint(vars(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = reddit.subreddits.search(\"t5_2r99w,t5_2r99w\")\n",
    "next(gen), next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = reddit.comment(id=\"dxolpyc\")\n",
    "# assume you have a Reddit instance bound to variable `reddit`\n",
    "# str(comment)\n",
    "# print(submission.title) # to make it non-lazy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#     print(comms[[not value for value in comms['nest_level'].isna()]].head())\n",
    "    for i, row in enumerate(comms[['nest_level', 'depth']].values):\n",
    "        print(i, all([np.isnan(val) for val in row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(comms[np.logical_and(np.isnan(comms['nest_level']), np.isnan(comms['depth']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the purposes of combining the depth and the nest_level, 750 comments have neither a depth nor a nest_level. They are all from trolls. With a few dozen exceptions, all have parent_ids starting with t3, meaning they have depth=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oops, parent_of_troll isn't working -- FIXED\n",
    "np.logical_and(comms['troll?']==1, comms['parent_of_troll?']==1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODONE\n",
    "* what is the human readable subreddit given the fullname, eg. t5_2r99w (DONE)\n",
    "```python\n",
    "gen = reddit.subreddits.search(\"t5_2r99w,t5_2r99w\")\n",
    "next(gen), next(gen)\n",
    "```\n",
    "```\n",
    "(Subreddit(display_name='whisky'),\n",
    " Subreddit(display_name='BaseballbytheNumbers'))\n",
    "```\n",
    "* get user link and comment karma (partially done - https://old.reddit.com/r/pushshift/comments/9i8s23/dataset_metadata_for_69_million_reddit_users_in/)\n",
    " - still missing karma for 623 userids\n",
    "```bash\n",
    "$ wc -l data/user_activity_dataframe*\n",
    "  3826 data/user_activity_dataframe.csv\n",
    "  3203 data/user_activity_dataframe_RA_2018-09.csv\n",
    "  3199 data/user_activity_dataframe_reddit_accounts.csv\n",
    " 10228 total\n",
    "```\n",
    "* how to get the permalink for each comment (DONE)\n",
    "```python\n",
    "comment = reddit.comment(id=\"dxolpyc\")\n",
    "comment.permalink\n",
    "```\n",
    "```\n",
    "'/r/redditdev/comments/8dmv8z/is_there_no_distinguish_method_for_comments_in/dxolpyc/'\n",
    "```\n",
    "* what is the author given its id? e.g. t2_105z8m (DONE)\n",
    "```python\n",
    "str(reddit.redditor(fullname=\"t2_z919g\"))\n",
    "```\n",
    "```\n",
    "'BlackToLive'\n",
    "```\n",
    "* is their greater controversiality in troll comments than others? (DONE)\n",
    "  - yes, slightly -- 4% to 3%\n",
    "* retrieve missing comments\n",
    "  - parents of trolls (missing 695, ~50%) (DONE)\n",
    "  - replies to trolls (missing ALL of them, unless the reply is from a troll) (DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
