{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import praw\n",
    "from prawcore import Forbidden\n",
    "from praw.exceptions import ClientException\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from pymongo import MongoClient, errors\n",
    "\n",
    "from bson.json_util import loads, dumps\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import boto3\n",
    "\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "import string\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = {}\n",
    "with open ('/opt/cap1/.cap1', 'r') as fp:\n",
    "    for line in fp:\n",
    "        k, v = line.replace('\\n','').split('\\t')\n",
    "        creds[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=creds['REDDIT_ID'], \n",
    "    client_secret=creds['REDDIT_SECRET'],\n",
    "    password=creds['REDDIT_PASSWORD'], \n",
    "    username=creds['REDDIT_USERNAME'],\n",
    "    user_agent='accessAPI:v0.0.1 (by /u/{})'.format(creds['REDDIT_USERNAME']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client['cap2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all comments into a list called 'docs', then make dataframe\n",
    "query = {}\n",
    "fields = { '_id': 0 }\n",
    "# docs is about 1.3 GB in RAM\n",
    "docs = list(db['comment'].find( query, fields ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = pd.DataFrame(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add submission ids to the set of ids from trolls, probably append 't1_' to comments\n",
    "# for ease of use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign labels, 'troll?', and 'child_of_troll?', and 'parent_of_troll?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of classes for target\n",
    "classes = ['other', 'parent', 'child', 'troll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6704"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign the label, 'troll?'' to each comment\n",
    "troll_comment_ids_set = set(comms[~comms['author'].isna()]['id'])\n",
    "comms['troll?'] = [int(mybool) for mybool in [\n",
    "                    commid in troll_comment_ids_set for commid in comms['id']\n",
    "                    ]]\n",
    "np.sum(comms['troll?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2985"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is this comment in reply to a troll?\n",
    "comms['child_of_troll?'] = [int(mybool) for mybool in [\n",
    "                    pid.split('_')[1] in troll_comment_ids_set for pid in comms['parent_id']\n",
    "                    ]]\n",
    "np.sum(comms['child_of_troll?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### only 57, praw never gave us children of troll comments...the only ones we have are from trolls replying to trolls\n",
    "\n",
    "#### FIXED, used praw to get the troll comments, then used .refresh() to load the replies. worked ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1824"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# did a troll reply to this?\n",
    "troll_parent_ids_set = set(\n",
    "                            [p.split('_')[1] for p in comms[~comms['author'].isna()]['parent_id']]\n",
    "                        )\n",
    "comms['parent_of_troll?'] = [int(myid in troll_parent_ids_set) for myid in comms['id']]\n",
    "\n",
    "np.sum(comms['parent_of_troll?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['t1', 't3'], dtype='<U2'), array([1839, 4348]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique([p.split('_')[0] for p in np.unique(comms[~comms['author'].isna()]['parent_id'])], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hm, 1144 comments are parents of trolls, but there are 1839 't1'-style parent_ids among troll comments. Which of the unique parent ids aren't getting labeled as parent?\n",
    "\n",
    "#### FIXED, got all of them except 15. Seems like reddit praw is a bit stingy when it comes to providing all comments in a thread, perhaps especially around trolls. Could make a test for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6187"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_ids = np.unique(comms[~comms['author'].isna()]['parent_id'])\n",
    "len(parent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1839, 1824)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_parent_ids=[]\n",
    "for p in parent_ids:\n",
    "    if p[:2]=='t1':\n",
    "        t1_parent_ids.append(p.split('_')[1])\n",
    "set_parent_ids = set(t1_parent_ids)\n",
    "set_parent_of_troll = set(comms[comms['parent_of_troll?']==1]['id'])\n",
    "# how does a differ from \n",
    "len(set_parent_ids), len(set_parent_of_troll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the 695 comment ids that probably need to be set as parent_of_troll\n",
    "missing_commentids = np.array(t1_parent_ids)[[myid not in set_parent_of_troll for myid in t1_parent_ids]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([15]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are these comments simply missing from the data? YES\n",
    "# FIXED, only 15 remain after starting with 1824 ish\n",
    "num_hits = []\n",
    "for cid in missing_commentids:\n",
    "    num_hits.append(len(comms[comms['id']==cid]))\n",
    "np.unique(num_hits, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_commentids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printall(pd_obj):\n",
    "    '''\n",
    "    print every row and column in a pandas object\n",
    "    '''\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(pd_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author                                                                           NaN\n",
      "author_created_utc                                                               NaN\n",
      "author_flair_css_class                                                    mod-rikker\n",
      "author_flair_text                  http://steamcommunity.com/profiles/76561198054...\n",
      "author_fullname                                                             t2_6ey3m\n",
      "body                               Prepare yourself for several \"FTL for Borderla...\n",
      "controversiality                                                                   0\n",
      "created_utc                                                              1.34787e+09\n",
      "distinguished                                                                   None\n",
      "gilded                                                                             0\n",
      "id                                                                           c69bqat\n",
      "link_id                                                                    t3_100ikq\n",
      "nest_level                                                                       NaN\n",
      "parent_id                                                                  t3_100ikq\n",
      "reply_delay                                                                      NaN\n",
      "retrieved_on                                                                     NaN\n",
      "score                                                                              5\n",
      "stickied                                                                       False\n",
      "subreddit                                                                        NaN\n",
      "subreddit_id                                                                t5_2skv6\n",
      "created                                                                   1.3479e+09\n",
      "edited                                                                         False\n",
      "score_hidden                                                                   False\n",
      "can_gild                                                                        True\n",
      "is_submitter                                                                   False\n",
      "no_follow                                                                      False\n",
      "permalink                          /r/SteamGameSwap/comments/100ikq/h_borderlands...\n",
      "send_replies                                                                    True\n",
      "subreddit_type                                                            restricted\n",
      "gildings                                                                          {}\n",
      "updated_utc                                                                      NaN\n",
      "author_flair_template_id                                                        None\n",
      "collapsed                                                                      False\n",
      "collapsed_reason                                                                None\n",
      "total_awards_received                                                              0\n",
      "approved_at_utc                                                                  NaN\n",
      "ups                                                                                5\n",
      "awarders                                                                          []\n",
      "mod_reason_by                                                                    NaN\n",
      "banned_by                                                                        NaN\n",
      "author_flair_type                                                               text\n",
      "removal_reason                                                                   NaN\n",
      "likes                                                                            NaN\n",
      "user_reports                                                                      []\n",
      "saved                                                                          False\n",
      "banned_at_utc                                                                    NaN\n",
      "mod_reason_title                                                                 NaN\n",
      "archived                                                                        True\n",
      "can_mod_post                                                                   False\n",
      "report_reasons                                                                   NaN\n",
      "approved_by                                                                      NaN\n",
      "all_awardings                                                                     []\n",
      "downs                                                                              0\n",
      "author_flair_richtext                                                             []\n",
      "author_patreon_flair                                                           False\n",
      "body_html                          <div class=\"md\"><p>Prepare yourself for severa...\n",
      "associated_award                                                                 NaN\n",
      "author_premium                                                                 False\n",
      "author_flair_text_color                                                         dark\n",
      "num_reports                                                                      NaN\n",
      "locked                                                                         False\n",
      "name                                                                      t1_c69bqat\n",
      "treatment_tags                                                                    []\n",
      "subreddit_name_prefixed                                              r/SteamGameSwap\n",
      "depth                                                                              0\n",
      "author_flair_background_color                                                       \n",
      "collapsed_because_crowd_control                                                  NaN\n",
      "mod_reports                                                                       []\n",
      "mod_note                                                                         NaN\n",
      "_fetched                                                                        True\n",
      "author_cakeday                                                                   NaN\n",
      "top_awarded_type                                                                 NaN\n",
      "troll?                                                                             0\n",
      "child_of_troll?                                                                    0\n",
      "parent_of_troll?                                                                   0\n",
      "Name: 6704, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print the first non-troll comment, making sure to \n",
    "r = np.where(comms['author'].isna())\n",
    "printall(comms.iloc[r[0], :].iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission    :  14523\n",
      "first_and_last:    964\n",
      "comment       : 234008\n"
     ]
    }
   ],
   "source": [
    "for coll in db.list_collection_names():\n",
    "    print(f'{coll:14}: {db[coll].count_documents({}):6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit's api changed, dict(vars(comment)) only worked after running a ._fetch()\n",
    "# get missing comments from pushshift\n",
    "# gen = api.search_comments(id=','.join(missing_commentids))\n",
    "# gen = api.search_comments(id='c55cs4v')\n",
    "ids =','.join(missing_commentids[:2])\n",
    "#print(ids)\n",
    "#gen = api.search_comments(id=ids)\n",
    "comment = reddit.comment(id=\"cqe9a6b\")\n",
    "pprint(vars(comment))\n",
    "#for c in gen:\n",
    "#    print(c.d_['body'])\n",
    "    #db['comment'].insert_one(c.d_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, so c55cs4v is here -- \n",
    "# https://snew.notabug.io/r/unixporn/comments/vjbgg/archlinux_dwm/c55cs4v/\n",
    "# and you can see shomyo's reply with permalink -- \n",
    "# https://snew.notabug.io/r/unixporn/comments/vjbgg/archlinux_dwm/c55hjc4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-up the parent comments, worked great.\n",
    "log = [] \n",
    "i = 0 # had clientexception at 59, id='cqe9a6b'\n",
    "for comment_id in missing_commentids[i:]: \n",
    "    i += 1 \n",
    "    print(f'searching for comment id: {comment_id}') \n",
    "    comment = reddit.comment(id=comment_id) \n",
    "    try: \n",
    "        comment._fetch() \n",
    "    except (Forbidden, ClientException): \n",
    "        log.append(i) \n",
    "        continue \n",
    "    d = dict(vars(comment)) \n",
    "    for key in ['_replies', '_submission', \n",
    "            '_reddit', 'mod', 'author', \n",
    "            'subreddit']: \n",
    "        _ = d.pop(key, None) \n",
    "    try: \n",
    "        db['comment'].insert_one(d) \n",
    "    except (errors.DuplicateKeyError, errors.InvalidDocument): \n",
    "        log.append[i] \n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission    :  14523\n",
      "first_and_last:    964\n",
      "comment       : 234008\n"
     ]
    }
   ],
   "source": [
    "for coll in db.list_collection_names():\n",
    "    print(f'{coll:14}: {db[coll].count_documents({}):6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the before and after numbers of records in mongo\n",
    "# 15 records failed\n",
    "num_records_added = 231533 - 230853\n",
    "num_records_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the ipython session in which I ran the scrape\n",
    "log = [59, 63, 84, 98, 109, 187, 500, 580, 632, 667, 670, 676, 678, 684, 689]\n",
    "missing_commentids[log]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cqhtj4q\n",
    "```\n",
    "[–]zenzog3 points 5 years ago \n",
    "https://mobile.twitter.com/shoxCSGO/status/588523685419601922\n",
    "permalink save report give gold reply\n",
    "```\n",
    "https://snew.notabug.io/r/csgobetting/comments/3324gu/envyus_vs_dignitas_bo3_190415_2100_cest/cqhtj4q/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d4ssri9\n",
    "```\n",
    "[–][censored]3 points 4 years ago \n",
    "[censored]\n",
    "permalink save report give gold reply[removed by moderators]\n",
    "```\n",
    "https://snew.notabug.io/r/rage/comments/4qgm68/woman_26_wielded_hatchet_after_her_demands_for/d4ssri9/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dp9fs27\n",
    "```\n",
    "–][deleted]2 points 3 years ago \n",
    "[deleted]\n",
    "permalink save report give gold reply\n",
    "```\n",
    "https://snew.notabug.io/r/Sissies/comments/7ad3vx/any_requests/dp9fs27/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cefixa8'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the replies to the trolls\n",
    "# get missing comments from pushshift\n",
    "troll_commentids = comms[~comms['author'].isna()]['id']\n",
    "np.random.choice(troll_commentids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-up the child comments, worked great\n",
    "log = [] \n",
    "i = 0 \n",
    "for comment_id in troll_commentids:  \n",
    "    i += 1  \n",
    "    print(f'searching for comment id: {comment_id}')  \n",
    "    comment = reddit.comment(id=comment_id)  \n",
    "    try:  \n",
    "        comment.refresh() \n",
    "    except (Forbidden, ClientException):  \n",
    "        log.append(i)  \n",
    "        continue \n",
    "    for reply in comment._replies: \n",
    "        d = dict(vars(reply)) \n",
    "        did = d['id'] \n",
    "        print(f'found reply id: {did}')  \n",
    "        for key in ['_replies', '_submission',  \n",
    "                '_reddit', 'mod', 'author',  \n",
    "                'subreddit']:  \n",
    "            _ = d.pop(key, None)  \n",
    "        try:  \n",
    "            db['comment'].insert_one(d) \n",
    "        except (errors.DuplicateKeyError, errors.InvalidDocument): \n",
    "            log.append(i)  \n",
    "            continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission    :  14523\n",
      "first_and_last:    964\n",
      "comment       : 234008\n"
     ]
    }
   ],
   "source": [
    "# 234008 comments after first child run, steady after re-run so far...30 min or so\n",
    "for coll in db.list_collection_names():\n",
    "    print(f'{coll:14}: {db[coll].count_documents({}):6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2475"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_added = 234008 - 231533\n",
    "records_added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_stopwords_ = set(\"a,able,about,across,after,all,almost,also,am,among,an,and,any,\\\n",
    "are,as,at,be,because,been,but,by,can,could,dear,did,do,does,either,\\\n",
    "else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,\\\n",
    "how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,\\\n",
    "me,might,most,must,my,neither,no,of,off,often,on,only,or,other,our,\\\n",
    "own,rather,said,say,says,she,should,since,so,some,than,that,the,their,\\\n",
    "them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,\\\n",
    "what,when,where,which,while,who,whom,why,will,with,would,yet,you,your,u,s\".split(','))\n",
    "\n",
    "# new stopwords after first run through\n",
    "# sws_to_add_brf1 = 'thanks,op,tie,deleted,game,crypto,invest,\\\n",
    "# edit,platform,play,giveaway,ok,yeah,blockchain,enter,remove,\\\n",
    "# m,token,awesome,ethereum,exchange,steam,dude'.split(',')\n",
    "\n",
    "# stopwords_ = set(list(orig_stopwords_) + sws_to_add_brf1)\n",
    "stopwords_ = orig_stopwords_\n",
    "\n",
    "punctuation_ = set(string.punctuation + '’' + '“')\n",
    "\n",
    "def rm_punctuation(a_string):\n",
    "    table = str.maketrans('', '', ''.join(punctuation_))\n",
    "    return a_string.translate(table)\n",
    "        \n",
    "def tokens_lower(tokens):\n",
    "    return [word.lower() for word in tokens]\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "#     for w in sent: print(w)\n",
    "#     if w == \"’\": print(\"this is ': \" + w)\n",
    "    return [w for w in tokens if not w in stopwords_]\n",
    "\n",
    "def stem_tokens(ntlk_stem_obj, tokens):\n",
    "    # visualize what stemming and lemmitization does!\n",
    "    # str(porter.__class__) = \"<class 'nltk.stem.porter.PorterStemmer'>\"\n",
    "#     name = str(ntlk_stem_lemm_obj.__class__).split(\"'\")[1].split('.')[-1]\n",
    "#     count_ident = 0\n",
    "#     count_alter = 0\n",
    "    return [ntlk_stem_obj.stem(tok) for tok in tokens]\n",
    "#     elif ''\n",
    "# for tok in tokens:\n",
    "#         return ntlk_stem_lemm_obj.stem(tok)\n",
    "#         if tok == stem_lemm_f_l_tok:\n",
    "#             count_ident += 1\n",
    "#         else:\n",
    "#             print(tok, stem_lemm_f_l_tok)\n",
    "#             count_alter += 1\n",
    "#     print('{}:\\nNumber of unchanged words: {}\\nchanged words: {}\\n'.format(name, count_ident, count_alter))\n",
    "\n",
    "def lemm_tokens(ntlk_lemm_obj, tokens):\n",
    "    return [ntlk_lemm_obj.lemmatize(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_pre_proc_doc(input_string, stemming=False):\n",
    "    '''\n",
    "    given document:\n",
    "        tokenizes the document\n",
    "        sets tokens to lower case\n",
    "        filters punctuation and stop words from tokens\n",
    "        returns porter, snowball, and wordnet stem/lemm tokens\n",
    "    '''\n",
    "    no_punct_string = rm_punctuation(input_string)\n",
    "    tokens = word_tokenize(no_punct_string)\n",
    "    lo_tokens = tokens_lower(tokens)\n",
    "    f_lo_tokens = filter_tokens(lo_tokens)\n",
    "    if stemming:\n",
    "        s_f_lo_tokens = stem_tokens(porter, f_lo_tokens)\n",
    "        le_s_f_lo_tokens = lemm_tokens(wordnet, s_f_lo_tokens)\n",
    "        return ' '.join(le_s_f_lo_tokens)\n",
    "    else:\n",
    "        le_f_lo_tokens = lemm_tokens(wordnet, f_lo_tokens)\n",
    "        return ' '.join(le_f_lo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a few minutes, and uses about 100 MB of RAM\n",
    "corpus = [nlp_pre_proc_doc(d) for d in comms['body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a dictionary of trained classifiers for comparison\n",
    "clfs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_corp, X_test_corp, y_train, y_test = train_test_split(\n",
    "        corpus, y, test_size=0.2, random_state=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 5000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_corp)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_tfidf.toarray()\n",
    "X_test = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "# at first I did 100 estimators, but 100*25 is only 2500 \n",
    "#  whereas we have 5000 features in tfidf. Increase to 400\n",
    "model_param = {'n_estimators': 400,\n",
    "                   'max_depth': 5,\n",
    "                   'max_features': 25,\n",
    "                   'oob_score': True,\n",
    "                   'n_jobs': -1,\n",
    "                   'random_state': 30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brf = RandomForestClassifier(**model_param, class_weight='balanced_subsample')\n",
    "brf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbrf = BalancedRandomForestClassifier(**model_param, class_weight='balanced_subsample')\n",
    "imbrf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_confusion_matrix(y_true, y_predict):\n",
    "    \"\"\"\n",
    "    y_true = [1, 1, 1, 1, 1, 0, 0]\n",
    "\n",
    "    y_predict = [1, 1, 1, 1, 0, 0, 0]\n",
    "\n",
    "    In [1]: standard_confusion_matrix(y_true, y_predict)\n",
    "    >> array([[4., 1.],\n",
    "    >>       [0., 2.]])\n",
    "    \"\"\"\n",
    "    cm = np.zeros((2,2))\n",
    "    X = np.array([y_true, y_predict])\n",
    "    values, counts = np.unique(X, axis=1, return_counts=True)\n",
    "    for i, v in enumerate(values.T):\n",
    "        cm[tuple([1, 1] - v)] = counts[i]\n",
    "    return cm.T.astype(int)\n",
    "\n",
    "# from the lecture\n",
    "# Just handy function to make our confusion matrix pretty \n",
    "def plot_confusion_matrix(cm, # confusion matrix\n",
    "                          classes_x, # test to describe what the output of the classes may be (commonly 1 or 0)\n",
    "                          classes_y,\n",
    "                          normalize=False, \n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes_x))\n",
    "    plt.xticks(tick_marks, classes_x, rotation=45)\n",
    "    plt.yticks(tick_marks, classes_y)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i,  format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Predicted label')\n",
    "    plt.xlabel('True label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_nofit(ax, X_test, y_test, clf, clf_name, **kwargs):\n",
    "    y_prob = np.zeros((len(y_test),2))\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    # Predict probabilities, not classes\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob[:, 1])\n",
    "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if len(ax.lines) == 0:\n",
    "        plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "    ax.plot(fpr, tpr, lw=1, label='%s (area = %0.2f)' % (clf_name, roc_auc))\n",
    "    mean_tpr /= 1\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "#     plt.plot(mean_fpr, mean_tpr, 'k--',label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the model for comparisons\n",
    "clfs['Balanced_RF'] = brf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(brf, X_test, y_test,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rnd_smp = np.random.random_sample(len(X)) < 0.\n",
    "#fig, ax = plt.subplots(1, figsize=(6, 5))\n",
    "#classifier_labels = ['Random_Forest (RF)', 'Balanced_RF']\n",
    "#classifiers = {'Random_Forest (RF)': rf, 'Balanced_RF': brf}\n",
    "#for label, clf in clfs.items():\n",
    "    #plot_roc_nofit(ax, X_test, y_test, clf, label)\n",
    "    #multi_class='ovo'\n",
    ">>> import numpy as np\n",
    ">>> from sklearn.metrics import roc_auc_score\n",
    ">>> y_true = np.array([0, 0, 1, 1])\n",
    ">>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    ">>> roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "troll_comments = comms[comms['troll?']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(troll_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the ids for all the troll_comments, then retrieve all the replies to it from Pushshift\n",
    "troll_comments['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = api.search_comments(parent_id='t1_dr3b6ce,t1_cpe9ci5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gen).d_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen = api.search_comments(id='cax4ng0,cax1t9x')\n",
    "next(gen).d_, next(gen).d_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so, let's recap the tomfoolery between praw and psaw\n",
    "0) psaw returns comments from trolls, praw does not ... true? FALSE, psaw doesn't give results when parent_id is from a troll\n",
    "1) psaw returns nothing when searching on threads with comments from trolls ... true? mostly. I believe it's not 100%, but e.g. https://api.pushshift.io/reddit/submission/comment_ids/4rdu5x returns []\n",
    "2) re: 1, reddit does, but does not include any responses to the trolls ... true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "* classify on pca / increase the number of maximum features per tree\n",
    "* data completeness\n",
    "  - get nest_level for everything\n",
    "  - do we have all comments multiple levels below troll comments?\n",
    "  - authors, subreddits, link- and comment-karma\n",
    "  - permalink\n",
    "* one-hot encode the subreddit to add to the tfidf\n",
    "* add user activity profile to the tfidf\n",
    "* start with id, (comment)\n",
    "  - is the comment trollish? proba > threshold\n",
    "  - if yes, then \n",
    "    - classify all other comments from the user (max=1000?)\n",
    "    - for each trollish comment, recurse on other trollish comments in its thread (max=1000?)\n",
    "  - given all the comments (including non-trollish from threads), view the author creation date and test for spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    troll_comments = comms[comms['troll?']==1]\n",
    "    print(troll_comments[troll_comments['subreddit']=='politics'])\n",
    "# comms.tail().iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    not_troll_comments = comms[comms['troll?']!=1]\n",
    "    print(not_troll_comments[not_troll_comments['subreddit_id']=='t5_2cneq'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = comms['class_label'].values\n",
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(comms['subreddit_id'], comms['subreddit']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(comms['subreddit_id'] + '_' + comms['subreddit'], return_counts=True)\n",
    "comms['subreddit_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join the [71.6 million usernames](https://old.reddit.com/r/pushshift/search?q=karma&restrict_sr=on&sort=relevance&t=all) to the user activity dataframe so we can efficiently load the link and comment karma of all these users into mongo\n",
    "```bash\n",
    "comm <(\\\n",
    "    sed '1d' user_activity_dataframe.csv | cut -d, -f2 | sort -u\\\n",
    "    ) \\\n",
    "    <(\\\n",
    "    cut -d, -f1 user_activity_dataframe_reddit_accounts.csv | sort -u\\\n",
    "    ) | \n",
    "awk -F\"\\t\" '{print NF;}' | sort | uniq -c\n",
    "```\n",
    "``` \n",
    "    56 1\n",
    "   277 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the user link- and comment-karma from reddit praw\n",
    "for item in reddit.redditor(\"Kevin_Milner\").downvoted():\n",
    "    print(item.id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit.redditor(\"spez\")\n",
    "reddit.user.me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.redditor(\"spez\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(reddit.redditor(fullname=\"t2_z919g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.subreddit(display_name=\"t5_2r99w\").subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = reddit.subreddits.search(\"t5_2r99w,t5_2r99w\")\n",
    "\n",
    "# assume you have a Reddit instance bound to variable `reddit`\n",
    "# str(reddit.submission(id=\"39zje0,\").subreddit)\n",
    "# print(submission.title) # to make it non-lazy\n",
    "# pprint.pprint(vars(submission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = reddit.subreddits.search(\"t5_2r99w,t5_2r99w\")\n",
    "next(gen), next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = reddit.comment(id=\"dxolpyc\")\n",
    "# assume you have a Reddit instance bound to variable `reddit`\n",
    "# str(comment)\n",
    "# print(submission.title) # to make it non-lazy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#     print(comms[[not value for value in comms['nest_level'].isna()]].head())\n",
    "    for i, row in enumerate(comms[['nest_level', 'depth']].values):\n",
    "        print(i, all([np.isnan(val) for val in row]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the purposes of combining the depth and the nest_level, 750 comments have neither a depth nor a nest_level. They are all from trolls. With a few dozen exceptions, all have parent_ids starting with t3, meaning they have depth=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oops, parent_of_troll isn't working -- FIXED\n",
    "np.logical_and(comms['troll?']==1, comms['parent_of_troll?']==1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODONE\n",
    "* what is the human readable subreddit given the fullname, eg. t5_2r99w (DONE)\n",
    "```python\n",
    "gen = reddit.subreddits.search(\"t5_2r99w,t5_2r99w\")\n",
    "next(gen), next(gen)\n",
    "```\n",
    "```\n",
    "(Subreddit(display_name='whisky'),\n",
    " Subreddit(display_name='BaseballbytheNumbers'))\n",
    "```\n",
    "* get user link and comment karma (partially done - https://old.reddit.com/r/pushshift/comments/9i8s23/dataset_metadata_for_69_million_reddit_users_in/)\n",
    " - still missing karma for 623 userids\n",
    "```bash\n",
    "$ wc -l data/user_activity_dataframe*\n",
    "  3826 data/user_activity_dataframe.csv\n",
    "  3203 data/user_activity_dataframe_RA_2018-09.csv\n",
    "  3199 data/user_activity_dataframe_reddit_accounts.csv\n",
    " 10228 total\n",
    "```\n",
    "* how to get the permalink for each comment (DONE)\n",
    "```python\n",
    "comment = reddit.comment(id=\"dxolpyc\")\n",
    "comment.permalink\n",
    "```\n",
    "```\n",
    "'/r/redditdev/comments/8dmv8z/is_there_no_distinguish_method_for_comments_in/dxolpyc/'\n",
    "```\n",
    "* what is the author given its id? e.g. t2_105z8m (DONE)\n",
    "```python\n",
    "str(reddit.redditor(fullname=\"t2_z919g\"))\n",
    "```\n",
    "```\n",
    "'BlackToLive'\n",
    "```\n",
    "* is their greater controversiality in troll comments than others? (DONE)\n",
    "  - yes, slightly -- 4% to 3%\n",
    "* retrieve missing comments\n",
    "  - parents of trolls (missing 695, ~50%) (DONE)\n",
    "  - replies to trolls (missing ALL of them, unless the reply is from a troll) (DONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getting nesting level, k\n",
    "currently there are 1430 comments with neither a 'nest_level' nor a 'depth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many of the no_depth_comms are responses to threads, so depth=0?\n",
    " -- first, though, is it true that all depths/nest_levels are 0 for parent_id~t3_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index into the rows with neither depth, nor nest_level\n",
    "no_depth_index = np.logical_and(np.isnan(comms['nest_level']), np.isnan(comms['depth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>nest_level</th>\n",
       "      <th>parent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>t1_d4quv2y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>t3_4q7hpd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>t1_d4quadd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>t1_d4qt7ql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>t1_d45nk4k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234003</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t1_d13d867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234004</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t1_d10j595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234005</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t1_d0zlio9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234006</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t1_crd86kr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234007</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t1_crcbcld</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232578 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        depth  nest_level   parent_id\n",
       "0         NaN         7.0  t1_d4quv2y\n",
       "1         NaN         1.0   t3_4q7hpd\n",
       "2         NaN         5.0  t1_d4quadd\n",
       "3         NaN         3.0  t1_d4qt7ql\n",
       "4         NaN         8.0  t1_d45nk4k\n",
       "...       ...         ...         ...\n",
       "234003    1.0         NaN  t1_d13d867\n",
       "234004    1.0         NaN  t1_d10j595\n",
       "234005    1.0         NaN  t1_d0zlio9\n",
       "234006    1.0         NaN  t1_crd86kr\n",
       "234007    1.0         NaN  t1_crcbcld\n",
       "\n",
       "[232578 rows x 3 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logical not the index to get the comments with one or the other\n",
    "has_depth_trimdf = comms[~no_depth_index][['depth', 'nest_level', 'parent_id']]\n",
    "has_depth_trimdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_typedf = pd.DataFrame(np.hstack([\n",
    "    has_depth_trimdf.values, \n",
    "    np.array([\n",
    "        x.split('_')[0] for x in has_depth_trimdf['parent_id'].values\n",
    "    ])\n",
    "    .reshape(-1, 1)\n",
    "]), columns=['depth', 'nest_level', 'parent_id', 'comm_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_typedf['depth'] = comm_typedf['depth'].astype(float)\n",
    "comm_typedf['nest_level'] = comm_typedf['nest_level'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">depth</th>\n",
       "      <th colspan=\"8\" halign=\"left\">nest_level</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comm_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t1</th>\n",
       "      <td>2475.0</td>\n",
       "      <td>1.369293</td>\n",
       "      <td>0.994013</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1697.0</td>\n",
       "      <td>3.279316</td>\n",
       "      <td>2.635883</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t3</th>\n",
       "      <td>224149.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4257.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              depth                                              nest_level  \\\n",
       "              count      mean       std  min  25%  50%  75%  max      count   \n",
       "comm_type                                                                     \n",
       "t1           2475.0  1.369293  0.994013  1.0  1.0  1.0  1.0  9.0     1697.0   \n",
       "t3         224149.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0     4257.0   \n",
       "\n",
       "                                                         \n",
       "               mean       std  min  25%  50%  75%   max  \n",
       "comm_type                                                \n",
       "t1         3.279316  2.635883  2.0  2.0  2.0  3.0  20.0  \n",
       "t3         1.000000  0.000000  1.0  1.0  1.0  1.0   1.0  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm_typedf.groupby('comm_type').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "omg, that was brutally difficult for some reason\n",
    "\n",
    "Confirmed offset between depth and nest_level\n",
    "\n",
    "if parent_id starts with 't3', the depth is always 0 and the nest_level is always 1\n",
    " -- nest_level = depth + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_depth_nest_value(df):\n",
    "    '''\n",
    "    if depth is not null, use it\n",
    "    else if nest_level is not null use (nest_level-1)\n",
    "    else use np.na\n",
    "    \n",
    "    returns df plus \n",
    "    '''\n",
    "\n",
    "def combine_nestlevel_depth_fillna(series):\n",
    "    '''\n",
    "    if depth is not null, use it\n",
    "    else if nest_level is not null use (nest_level-1)\n",
    "    else use 0 if parent_id starts with t3\n",
    "    else use median value of depth and (nest_level-1) \n",
    "    '''\n",
    "    median_depth = np.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "depth                NaN\n",
       "nest_level             7\n",
       "parent_id     t1_d4quv2y\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_depth_trimdf.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
