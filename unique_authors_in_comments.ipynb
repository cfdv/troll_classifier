{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "import praw\n",
    "from prawcore import Forbidden\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from pymongo import MongoClient, errors\n",
    "from bson.json_util import loads, dumps\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import boto3\n",
    "\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_curve, auc\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from scipy import interp\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client['cap2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all comments into a list called 'docs', then make dataframe\n",
    "query = {}\n",
    "fields = { '_id': 0 }\n",
    "# docs is about 1.3 GB in RAM\n",
    "docs = list(db['comment'].find( query, fields ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = pd.DataFrame(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add submission ids to the set of ids from trolls, probably append 't1_' to comments\n",
    "# for ease of use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assign labels, 'troll?', and 'child_of_troll?', and 'parent_of_troll?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6704"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign the label, 'troll?'' to each comment\n",
    "troll_comment_ids_set = set(comms[~comms['author'].isna()]['id'])\n",
    "comms['troll?'] = [int(mybool) for mybool in [\n",
    "                    commid in troll_comment_ids_set for commid in comms['id']\n",
    "                    ]]\n",
    "np.sum(comms['troll?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is this comment in reply to a troll?\n",
    "comms['child_of_troll?'] = [int(mybool) for mybool in [\n",
    "                    pid.split('_')[1] in troll_comment_ids_set for pid in comms['parent_id']\n",
    "                    ]]\n",
    "np.sum(comms['child_of_troll?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219019"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# did a troll reply to this?\n",
    "troll_parent_ids_set = set(\n",
    "                            [p.split('_')[1] for p in comms[~comms['author'].isna()]['parent_id']]\n",
    "                        )\n",
    "comms['parent_of_troll?'] = [int(mybool) for mybool in [\n",
    "                    parentid.split('_')[1] in troll_parent_ids_set for parentid in comms['parent_id']\n",
    "                    ]]\n",
    "np.sum(comms['parent_of_troll?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230853"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create array of classes for target\n",
    "y = comms[['troll?', 'child_of_troll?', 'parent_of_troll?']].values\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_stopwords_ = set(\"a,able,about,across,after,all,almost,also,am,among,an,and,any,\\\n",
    "are,as,at,be,because,been,but,by,can,could,dear,did,do,does,either,\\\n",
    "else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,\\\n",
    "how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,\\\n",
    "me,might,most,must,my,neither,no,of,off,often,on,only,or,other,our,\\\n",
    "own,rather,said,say,says,she,should,since,so,some,than,that,the,their,\\\n",
    "them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,\\\n",
    "what,when,where,which,while,who,whom,why,will,with,would,yet,you,your,u,s\".split(','))\n",
    "\n",
    "# new stopwords after first run through\n",
    "# sws_to_add_brf1 = 'thanks,op,tie,deleted,game,crypto,invest,\\\n",
    "# edit,platform,play,giveaway,ok,yeah,blockchain,enter,remove,\\\n",
    "# m,token,awesome,ethereum,exchange,steam,dude'.split(',')\n",
    "\n",
    "# stopwords_ = set(list(orig_stopwords_) + sws_to_add_brf1)\n",
    "stopwords_ = orig_stopwords_\n",
    "\n",
    "punctuation_ = set(string.punctuation + '’' + '“')\n",
    "\n",
    "def rm_punctuation(a_string):\n",
    "    table = str.maketrans('', '', ''.join(punctuation_))\n",
    "    return a_string.translate(table)\n",
    "        \n",
    "def tokens_lower(tokens):\n",
    "    return [word.lower() for word in tokens]\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "#     for w in sent: print(w)\n",
    "#     if w == \"’\": print(\"this is ': \" + w)\n",
    "    return [w for w in tokens if not w in stopwords_]\n",
    "\n",
    "def stem_tokens(ntlk_stem_obj, tokens):\n",
    "    # visualize what stemming and lemmitization does!\n",
    "    # str(porter.__class__) = \"<class 'nltk.stem.porter.PorterStemmer'>\"\n",
    "#     name = str(ntlk_stem_lemm_obj.__class__).split(\"'\")[1].split('.')[-1]\n",
    "#     count_ident = 0\n",
    "#     count_alter = 0\n",
    "    return [ntlk_stem_obj.stem(tok) for tok in tokens]\n",
    "#     elif ''\n",
    "# for tok in tokens:\n",
    "#         return ntlk_stem_lemm_obj.stem(tok)\n",
    "#         if tok == stem_lemm_f_l_tok:\n",
    "#             count_ident += 1\n",
    "#         else:\n",
    "#             print(tok, stem_lemm_f_l_tok)\n",
    "#             count_alter += 1\n",
    "#     print('{}:\\nNumber of unchanged words: {}\\nchanged words: {}\\n'.format(name, count_ident, count_alter))\n",
    "\n",
    "def lemm_tokens(ntlk_lemm_obj, tokens):\n",
    "    return [ntlk_lemm_obj.lemmatize(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_pre_proc_doc(input_string, stemming=False):\n",
    "    '''\n",
    "    given document:\n",
    "        tokenizes the document\n",
    "        sets tokens to lower case\n",
    "        filters punctuation and stop words from tokens\n",
    "        returns porter, snowball, and wordnet stem/lemm tokens\n",
    "    '''\n",
    "    no_punct_string = rm_punctuation(input_string)\n",
    "    tokens = word_tokenize(no_punct_string)\n",
    "    lo_tokens = tokens_lower(tokens)\n",
    "    f_lo_tokens = filter_tokens(lo_tokens)\n",
    "    if stemming:\n",
    "        s_f_lo_tokens = stem_tokens(porter, f_lo_tokens)\n",
    "        le_s_f_lo_tokens = lemm_tokens(wordnet, s_f_lo_tokens)\n",
    "        return ' '.join(le_s_f_lo_tokens)\n",
    "    else:\n",
    "        le_f_lo_tokens = lemm_tokens(wordnet, f_lo_tokens)\n",
    "        return ' '.join(le_f_lo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a few minutes, and uses about 100 MB of RAM\n",
    "corpus = [nlp_pre_proc_doc(d) for d in comms['body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_corp, X_test_corp, y_train, y_test = train_test_split(\n",
    "        corpus, y, test_size=0.2, random_state=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(corpus)\n",
    "n_features = 5000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a dictionary of trained classifiers for comparison\n",
    "clfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
